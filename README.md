
# Semantic Search within Video using CLIP Model

This project demonstrates a semantic search system implemented within video content using OpenAI's CLIP model. By leveraging CLIP's ability to understand both images and text, this system allows you to extract visual features from video frames and perform text-based queries to find relevant moments in the video.

## Features

- Uses the CLIP model to encode and compare text and video frames.
- Supports semantic search over video content.
- Extracts video frames and processes them for feature extraction.
- Designed for video indexing and retrieval tasks.

## Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/your-username/semantic-search-video-clip.git
   cd semantic-search-video-clip
